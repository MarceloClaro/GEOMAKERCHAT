from langchain.chains.conversation.memory import ConversationBufferWindowMemory
import os
import streamlit as st
from crewai import Agent, Task, Crew, Process
from langchain_groq import ChatGroq
from langchain_community.tools import DuckDuckGoSearchRun
import groq
import toml
import time

# Carregar a chave de API do Groq do arquivo secrets.toml
secrets = toml.load("secrets.toml")
groq_api_key = secrets["GROQ_API_KEY"]

# Definir os limites de taxa para cada modelo
rate_limits = {
    "llama3-70b-8192": 6000,
    "llama3-8b-8192": 30000,
    "gemma-7b-it": 15000,
    "mixtral-8x7b-32768": 5000
}

# Inicializar um dicionÃ¡rio para rastrear os tokens usados por cada modelo
tokens_used = {model: 0 for model in rate_limits}

def main():
    # ConfiguraÃ§Ãµes da pÃ¡gina do Streamlit
    st.set_page_config(page_icon="ðŸ’¬", layout="wide", page_title="Interface de Chat AvanÃ§ado com RAG+CreWAI")
    st.image("Untitled.png", width=100)
    st.title("Bem-vindo ao Chat Geomaker AvanÃ§ado com RAG+CreWAI!")
    st.write("""Este chatbot utiliza um modelo avanÃ§ado que combina geraÃ§Ã£o de linguagem com recuperaÃ§Ã£o de informaÃ§Ãµes.
    
    Com 3 Agentes: 
    
    Pesquisador AcadÃªmico": Encontre informaÃ§Ãµes confiÃ¡veis e atuais sobre tÃ³pico, seguindo as normas cientÃ­ficas e da ABNT.
    Como pesquisador acadÃªmico, seu objetivo Ã© contribuir para o avanÃ§o do conhecimento cientÃ­fico em sua Ã¡rea. 
    VocÃª segue rigorosamente as normas e metodologias cientÃ­ficas e da ABNT para garantir a qualidade e confiabilidade de suas pesquisas.
    Sua busca por informaÃ§Ãµes Ã© guiada pela busca da verdade e pela contribuiÃ§Ã£o para a comunidade acadÃªmica.
    Pesquise e compile informaÃ§Ãµes relevantes e atualizadas sobre o assunto, seguindo as normas cientÃ­ficas e da formataÃ§Ã£o ABNT. 
    Certifique-se de incluir referÃªncias bibliogrÃ¡ficas adequadas.
    Um resumo detalhado e bem estruturado sobre o tema, seguindo as normas cientÃ­ficas e nas normais da ABNT.
    
    Escritor de Artigos": Escreva conteÃºdos envolventes sobre {topic}.
    Como escritor de artigos, sua habilidade em transformar assuntos complexos em narrativas envolventes Ã© excepcional. 
    Sua escrita ilumina novas perspectivas e descobertas, tornando-as acessÃ­veis para todos. 
    AtravÃ©s do seu trabalho, vocÃª destaca os aspectos mais importantes das Ãºltimas tendÃªncias em diversos temas, tornando o mundo intricado das notÃ­cias uma jornada fascinante para seus leitores.

    Avaliador de Artigos": Avalie criticamente artigos acadÃªmicos sobre {topic}.
    Como avaliador de artigos, vocÃª possui habilidades analÃ­ticas aguÃ§adas e um profundo entendimento do processo de pesquisa acadÃªmica. 
    Sua anÃ¡lise crÃ­tica destaca nÃ£o apenas os pontos fortes, mas tambÃ©m as falhas potenciais nos artigos, contribuindo para a melhoria contÃ­nua da qualidade da pesquisa acadÃªmica.
    
    """)

    # Sidebar para customizaÃ§Ã£o
    primary_prompt = st.sidebar.text_input("Prompt do sistema principal", "Como posso ajudar vocÃª hoje?")
    secondary_prompt = st.sidebar.text_input("Prompt do sistema secundÃ¡rio", "HÃ¡ algo mais em que posso ajudar?")
    model_choice = st.sidebar.selectbox("Escolha um modelo", ["llama3-70b-8192", "llama3-8b-8192", "mixtral-8x7b-32768", "gemma-7b-it"])
    conversational_memory_length = st.sidebar.slider('Tamanho da memÃ³ria conversacional', 1, 50, value=5)

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    search_tool = DuckDuckGoSearchRun()
    academic_researcher = Agent(
        role="å­¦æœ¯ç ”ç©¶å‘˜",
        goal="åœ¨ç¬¦åˆç§‘å­¦è§„èŒƒå’ŒABNTè§„èŒƒçš„å‰æä¸‹ï¼Œæ‰¾åˆ°å…³äºŽ{topic}çš„å¯é å’Œæœ€æ–°ä¿¡æ¯",
        verbose=True,
        memory=True,
        backstory=(
            "ä½œä¸ºå­¦æœ¯ç ”ç©¶å‘˜ï¼Œæ‚¨çš„ç›®æ ‡æ˜¯ä¸ºæŽ¨è¿›æ‚¨é¢†åŸŸçš„çŸ¥è¯†å‘å±•åšå‡ºè´¡çŒ®ã€‚æ‚¨ä¸¥æ ¼éµå¾ªç§‘å­¦å’ŒABNTçš„è§„èŒƒå’Œæ–¹æ³•ï¼Œä»¥ç¡®ä¿æ‚¨çš„ç ”ç©¶è´¨é‡å’Œå¯é æ€§ã€‚æ‚¨å¯¹ä¿¡æ¯çš„æœç´¢æ˜¯ç”±å¯¹çœŸç†çš„è¿½æ±‚å’Œå¯¹å­¦æœ¯ç•Œçš„è´¡çŒ®é©±åŠ¨çš„ã€‚"
        ),
        tools=[search_tool],
        allow_delegation=True,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice)
    )

    blog_writer = Agent(
        role="æ–‡ç« æ’°å†™è€…",
        goal="æ’°å†™æœ‰å…³{topic}çš„å¼•äººå…¥èƒœçš„å†…å®¹",
        verbose=True,
        memory=True,
        backstory=(
            "ä½œä¸ºæ–‡ç« æ’°å†™è€…ï¼Œæ‚¨å°†å¤æ‚çš„ä¸»é¢˜è½¬åŒ–ä¸ºå¼•äººå…¥èƒœçš„å™è¿°çš„èƒ½åŠ›æ˜¯å¼‚å¸¸çš„ã€‚æ‚¨çš„å†™ä½œä¸ºæ–°è§†è§’å’Œå‘çŽ°æä¾›äº†å…‰æ˜Žï¼Œä½¿å®ƒä»¬å¯¹æ‰€æœ‰äººéƒ½æ›´æ˜“äºŽç†è§£ã€‚é€šè¿‡æ‚¨çš„å·¥ä½œï¼Œæ‚¨çªå‡ºäº†å„ç§ä¸»é¢˜æœ€æ–°è¶‹åŠ¿çš„æœ€é‡è¦æ–¹é¢ï¼Œä½¿æ–°é—»å¤æ‚çš„ä¸–ç•Œå¯¹æ‚¨çš„è¯»è€…æ¥è¯´æ˜¯ä¸€æ¬¡è¿·äººçš„æ—…ç¨‹ã€‚"
        ),
        tools=[search_tool],
        allow_delegation=False,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice)
    )

    article_evaluator = Agent(
        role="æ–‡ç« è¯„ä¼°è€…",
        goal="å¯¹{topic}çš„å­¦æœ¯æ–‡ç« è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä»·",
        verbose=True,
        memory=True,
        backstory=(
            "ä½œä¸ºæ–‡ç« è¯„ä¼°è€…ï¼Œæ‚¨å…·æœ‰æ•é”çš„åˆ†æžèƒ½åŠ›å’Œå¯¹å­¦æœ¯ç ”ç©¶è¿‡ç¨‹çš„æ·±åˆ»ç†è§£ã€‚æ‚¨çš„æ‰¹åˆ¤æ€§åˆ†æžä¸ä»…çªå‡ºäº†æ–‡ç« çš„ä¼˜ç‚¹ï¼Œè¿˜æŒ‡å‡ºäº†æ–‡ç« å¯èƒ½å­˜åœ¨çš„ç¼ºé™·ï¼Œä¸ºæŒç»­æ”¹è¿›å­¦æœ¯ç ”ç©¶è´¨é‡åšå‡ºäº†è´¡çŒ®ã€‚"
        ),
        tools=[search_tool],
        allow_delegation=False,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice)
    )

    research_task = Task(
        description=(
            "åœ¨ç¬¦åˆç§‘å­¦è§„èŒƒå’ŒABNTæ ¼å¼çš„å‰æä¸‹ï¼Œæœç´¢å’Œæ•´ç†æœ‰å…³{topic}çš„ç›¸å…³å’Œæœ€æ–°ä¿¡æ¯ã€‚ç¡®ä¿åŒ…å«é€‚å½“çš„å‚è€ƒæ–‡çŒ®ã€‚"
        ),
        expected_output="ä¸€ä»½è¯¦ç»†å’Œç»“æž„è‰¯å¥½çš„å…³äºŽ{topic}çš„æ‘˜è¦ï¼Œéµå¾ªç§‘å­¦è§„èŒƒå’ŒABNTè§„èŒƒã€‚",
        tools=[search_tool],
        agent=academic_researcher
    )

    write_task = Task(
        description=(
            "æ’°å†™æœ‰å…³{topic}çš„å¼•äººå…¥èƒœçš„å†…å®¹ï¼Œç€é‡ä»‹ç»æœ€æ–°è¶‹åŠ¿åŠå…¶å¯¹è¡Œä¸šçš„å½±å“ã€‚è¿™ç¯‡æ–‡ç« åº”æ˜“äºŽç†è§£ã€å¼•äººå…¥èƒœä¸”ç§¯æžå‘ä¸Šã€‚"
        ),
        expected_output="ä¸€ç¯‡å…³äºŽ{topic}è¿›å±•çš„å››æ®µæ–‡ç« ï¼Œé‡‡ç”¨markdownæ ¼å¼ã€‚",
        tools=[search_tool],
        agent=blog_writer,
        async_execution=False,
        output_file="blog-post.md"
    )

    evaluate_task = Task(
        description=(
            "å¯¹{topic}çš„å­¦æœ¯æ–‡ç« è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä»·ã€‚çªå‡ºæ–‡ç« çš„ä¼˜ç‚¹å’Œå¯èƒ½å­˜åœ¨çš„ç¼ºé™·ã€‚"
        ),
        expected_output="ä¸€ä»½å¯¹{topic}çš„å­¦æœ¯æ–‡ç« è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä»·ï¼Œçªå‡ºæ–‡ç« çš„ä¼˜ç‚¹å’Œå¯èƒ½å­˜åœ¨çš„ç¼ºé™·ã€‚",
        tools=[search_tool],
        agent=article_evaluator
    )

    crew = Crew(
        agents=[academic_researcher, blog_writer, article_evaluator],
        tasks=[research_task, write_task, evaluate_task],
        process=Process.sequential
    )

    user_question = st.text_input("FaÃ§a uma pergunta:")
    if user_question:
        current_prompt = secondary_prompt if 'last_prompt' in st.session_state and st.session_state.last_prompt == primary_prompt else primary_prompt
        st.session_state.last_prompt = current_prompt

        prompt = f"{current_prompt} {user_question}"
        model = crew.agents[0].llm.model_name  # Assume que o modelo do primeiro agente Ã© o modelo escolhido
        while True:
            # Verificar se excedeu o limite de tokens
            if tokens_used[model] >= rate_limits[model]:
                st.warning(f"Limite de tokens excedido para o modelo {model}. Aguardando antes de tentar novamente...")
                time.sleep(60)
                tokens_used[model] = 0
            else:
                try:
                    result = crew.kickoff(inputs={"topic": user_question})
                    st.write("Chatbot:", result)
                    tokens_used[model] += 1  # Incrementar o contador de tokens
                    break
                except groq.RateLimitError as e:
                    retry_time_str = None
                    if isinstance(e.args[0], dict) and "error" in e.args[0]:
                        retry_time_str = e.args[0]["error"]
                    else:
                        retry_time_str = "Tempo de espera desconhecido"
                    st.warning(f"Limite de taxa excedido. Aguardando {retry_time_str} segundos antes de tentar novamente...")
                    time.sleep(retry_time_str)

if __name__ == "__main__":
    main()
