from langchain.chains.conversation.memory import ConversationBufferWindowMemory
import os
import streamlit as st
from crewai import Agent, Task, Crew, Process
from langchain_groq import ChatGroq
from langchain_community.tools import DuckDuckGoSearchRun
import groq
import toml
import time

# Carregar a chave de API do Groq do arquivo secrets.toml
secrets = toml.load("secrets.toml")
groq_api_key = secrets["GROQ_API_KEY"]

# Definir os limites de taxa para cada modelo
rate_limits = {
    "llama3-70b-8192": 6000,
    "llama3-8b-8192": 30000,
    "gemma-7b-it": 15000,
    "mixtral-8x7b-32768": 5000
}

# Inicializar um dicion√°rio para rastrear os tokens usados por cada modelo
tokens_used = {model: 0 for model in rate_limits}

def main():
    # Configura√ß√µes da p√°gina do Streamlit
    st.set_page_config(page_icon="üí¨", layout="wide", page_title="Interface de Chat Avan√ßado com RAG+CreWAI")
    st.image("Untitled.png", width=100)
    st.title("Bem-vindo ao Chat Geomaker Avan√ßado com RAG+CreWAI!")
    st.write("""Este chatbot utiliza um modelo avan√ßado que combina gera√ß√£o de linguagem com recupera√ß√£o de informa√ß√µes.
    
    Com 3 Agentes: 
    
    Pesquisador Acad√™mico": Encontre informa√ß√µes confi√°veis e atuais sobre t√≥pico, seguindo as normas cient√≠ficas e da ABNT.
    Como pesquisador acad√™mico, seu objetivo √© contribuir para o avan√ßo do conhecimento cient√≠fico em sua √°rea. 
    Voc√™ segue rigorosamente as normas e metodologias cient√≠ficas e da ABNT para garantir a qualidade e confiabilidade de suas pesquisas.
    Sua busca por informa√ß√µes √© guiada pela busca da verdade e pela contribui√ß√£o para a comunidade acad√™mica.
    Pesquise e compile informa√ß√µes relevantes e atualizadas sobre o assunto, seguindo as normas cient√≠ficas e da formata√ß√£o ABNT. 
    Certifique-se de incluir refer√™ncias bibliogr√°ficas adequadas.
    Um resumo detalhado e bem estruturado sobre o tema, seguindo as normas cient√≠ficas e nas normais da ABNT.
    
    Escritor de Artigos": Escreva conte√∫dos envolventes sobre {topic}.
    Como escritor de artigos, sua habilidade em transformar assuntos complexos em narrativas envolventes √© excepcional. 
    Sua escrita ilumina novas perspectivas e descobertas, tornando-as acess√≠veis para todos. 
    Atrav√©s do seu trabalho, voc√™ destaca os aspectos mais importantes das √∫ltimas tend√™ncias em diversos temas, tornando o mundo intricado das not√≠cias uma jornada fascinante para seus leitores.

    Avaliador de Artigos": Avalie criticamente artigos acad√™micos sobre {topic}.
    Como avaliador de artigos, voc√™ possui habilidades anal√≠ticas agu√ßadas e um profundo entendimento do processo de pesquisa acad√™mica. 
    Sua an√°lise cr√≠tica destaca n√£o apenas os pontos fortes, mas tamb√©m as falhas potenciais nos artigos, contribuindo para a melhoria cont√≠nua da qualidade da pesquisa acad√™mica.
    
    """)

    # Sidebar para customiza√ß√£o
    primary_prompt = st.sidebar.text_input("Prompt do sistema principal", "Como posso ajudar voc√™ hoje?")
    secondary_prompt = st.sidebar.text_input("Prompt do sistema secund√°rio", "H√° algo mais em que posso ajudar?")
    model_choice = st.sidebar.selectbox("Escolha um modelo", ["llama3-70b-8192", "llama3-8b-8192", "mixtral-8x7b-32768", "gemma-7b-it"])
    conversational_memory_length = st.sidebar.slider('Tamanho da mem√≥ria conversacional', 1, 50, value=5)

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    search_tool = DuckDuckGoSearchRun()
    academic_researcher = Agent(
        role="Â≠¶ÊúØÁ†îÁ©∂Âëò",
        goal="Âú®Á¨¶ÂêàÁßëÂ≠¶ËßÑËåÉÂíåABNTËßÑËåÉÁöÑÂâçÊèê‰∏ãÔºåÊâæÂà∞ÂÖ≥‰∫é{topic}ÁöÑÂèØÈù†ÂíåÊúÄÊñ∞‰ø°ÊÅØ",
        verbose=True,
        memory=True,
        backstory=(
            "‰Ωú‰∏∫Â≠¶ÊúØÁ†îÁ©∂ÂëòÔºåÊÇ®ÁöÑÁõÆÊ†áÊòØ‰∏∫Êé®ËøõÊÇ®È¢ÜÂüüÁöÑÁü•ËØÜÂèëÂ±ïÂÅöÂá∫Ë¥°ÁåÆ„ÄÇÊÇ®‰∏•Ê†ºÈÅµÂæ™ÁßëÂ≠¶ÂíåABNTÁöÑËßÑËåÉÂíåÊñπÊ≥ïÔºå‰ª•Á°Æ‰øùÊÇ®ÁöÑÁ†îÁ©∂Ë¥®ÈáèÂíåÂèØÈù†ÊÄß„ÄÇÊÇ®ÂØπ‰ø°ÊÅØÁöÑÊêúÁ¥¢ÊòØÁî±ÂØπÁúüÁêÜÁöÑËøΩÊ±ÇÂíåÂØπÂ≠¶ÊúØÁïåÁöÑË¥°ÁåÆÈ©±Âä®ÁöÑ„ÄÇ"
        ),
        tools=[search_tool],
        allow_delegation=True,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice)
    )

    blog_writer = Agent(
        role="ÊñáÁ´†Êí∞ÂÜôËÄÖ",
        goal="Êí∞ÂÜôÊúâÂÖ≥{topic}ÁöÑÂºï‰∫∫ÂÖ•ËÉúÁöÑÂÜÖÂÆπ",
        verbose=True,
        memory=True,
        backstory=(
            "‰Ωú‰∏∫ÊñáÁ´†Êí∞ÂÜôËÄÖÔºåÊÇ®Â∞ÜÂ§çÊùÇÁöÑ‰∏ªÈ¢òËΩ¨Âåñ‰∏∫Âºï‰∫∫ÂÖ•ËÉúÁöÑÂèôËø∞ÁöÑËÉΩÂäõÊòØÂºÇÂ∏∏ÁöÑ„ÄÇÊÇ®ÁöÑÂÜô‰Ωú‰∏∫Êñ∞ËßÜËßíÂíåÂèëÁé∞Êèê‰æõ‰∫ÜÂÖâÊòéÔºå‰ΩøÂÆÉ‰ª¨ÂØπÊâÄÊúâ‰∫∫ÈÉΩÊõ¥Êòì‰∫éÁêÜËß£„ÄÇÈÄöËøáÊÇ®ÁöÑÂ∑•‰ΩúÔºåÊÇ®Á™ÅÂá∫‰∫ÜÂêÑÁßç‰∏ªÈ¢òÊúÄÊñ∞Ë∂ãÂäøÁöÑÊúÄÈáçË¶ÅÊñπÈù¢Ôºå‰ΩøÊñ∞ÈóªÂ§çÊùÇÁöÑ‰∏ñÁïåÂØπÊÇ®ÁöÑËØªËÄÖÊù•ËØ¥ÊòØ‰∏ÄÊ¨°Ëø∑‰∫∫ÁöÑÊóÖÁ®ã„ÄÇ"
        ),
        tools=[search_tool],
        allow_delegation=False,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice)
    )

    article_evaluator = Agent(
        role="ÊñáÁ´†ËØÑ‰º∞ËÄÖ",
        goal="ÂØπ{topic}ÁöÑÂ≠¶ÊúØÊñáÁ´†ËøõË°åÊâπÂà§ÊÄßËØÑ‰ª∑",
        verbose=True,
        memory=True,
        backstory=(
            "‰Ωú‰∏∫ÊñáÁ´†ËØÑ‰º∞ËÄÖÔºåÊÇ®ÂÖ∑ÊúâÊïèÈîêÁöÑÂàÜÊûêËÉΩÂäõÂíåÂØπÂ≠¶ÊúØÁ†îÁ©∂ËøáÁ®ãÁöÑÊ∑±ÂàªÁêÜËß£„ÄÇÊÇ®ÁöÑÊâπÂà§ÊÄßÂàÜÊûê‰∏ç‰ªÖÁ™ÅÂá∫‰∫ÜÊñáÁ´†ÁöÑ‰ºòÁÇπÔºåËøòÊåáÂá∫‰∫ÜÊñáÁ´†ÂèØËÉΩÂ≠òÂú®ÁöÑÁº∫Èô∑Ôºå‰∏∫ÊåÅÁª≠ÊîπËøõÂ≠¶ÊúØÁ†îÁ©∂Ë¥®ÈáèÂÅöÂá∫‰∫ÜË¥°ÁåÆ„ÄÇ"
        ),
        tools=[search_tool],
        allow_delegation=False,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice)
    )

    research_task = Task(
        description=(
            "Âú®Á¨¶ÂêàÁßëÂ≠¶ËßÑËåÉÂíåABNTÊ†ºÂºèÁöÑÂâçÊèê‰∏ãÔºåÊêúÁ¥¢ÂíåÊï¥ÁêÜÊúâÂÖ≥{topic}ÁöÑÁõ∏ÂÖ≥ÂíåÊúÄÊñ∞‰ø°ÊÅØ„ÄÇÁ°Æ‰øùÂåÖÂê´ÈÄÇÂΩìÁöÑÂèÇËÄÉÊñáÁåÆ„ÄÇ"
        ),
        expected_output="‰∏Ä‰ªΩËØ¶ÁªÜÂíåÁªìÊûÑËâØÂ•ΩÁöÑÂÖ≥‰∫é{topic}ÁöÑÊëòË¶ÅÔºåÈÅµÂæ™ÁßëÂ≠¶ËßÑËåÉÂíåABNTËßÑËåÉ„ÄÇ",
        tools=[search_tool],
        agent=academic_researcher
    )

    write_task = Task(
        description=(
            "Êí∞ÂÜôÊúâÂÖ≥{topic}ÁöÑÂºï‰∫∫ÂÖ•ËÉúÁöÑÂÜÖÂÆπÔºåÁùÄÈáç‰ªãÁªçÊúÄÊñ∞Ë∂ãÂäøÂèäÂÖ∂ÂØπË°å‰∏öÁöÑÂΩ±Âìç„ÄÇËøôÁØáÊñáÁ´†Â∫îÊòì‰∫éÁêÜËß£„ÄÅÂºï‰∫∫ÂÖ•ËÉú‰∏îÁßØÊûÅÂêë‰∏ä„ÄÇ"
        ),
        expected_output="‰∏ÄÁØáÂÖ≥‰∫é{topic}ËøõÂ±ïÁöÑÂõõÊÆµÊñáÁ´†ÔºåÈááÁî®markdownÊ†ºÂºè„ÄÇ",
        tools=[search_tool],
        agent=blog_writer,
        async_execution=False,
        output_file="blog-post.md"
    )

    evaluate_task = Task(
        description=(
            "ÂØπ{topic}ÁöÑÂ≠¶ÊúØÊñáÁ´†ËøõË°åÊâπÂà§ÊÄßËØÑ‰ª∑„ÄÇÁ™ÅÂá∫ÊñáÁ´†ÁöÑ‰ºòÁÇπÂíåÂèØËÉΩÂ≠òÂú®ÁöÑÁº∫Èô∑„ÄÇ"
        ),
        expected_output="‰∏Ä‰ªΩÂØπ{topic}ÁöÑÂ≠¶ÊúØÊñáÁ´†ËøõË°åÊâπÂà§ÊÄßËØÑ‰ª∑ÔºåÁ™ÅÂá∫ÊñáÁ´†ÁöÑ‰ºòÁÇπÂíåÂèØËÉΩÂ≠òÂú®ÁöÑÁº∫Èô∑„ÄÇ",
        tools=[search_tool],
        agent=article_evaluator
    )

    crew = Crew(
        agents=[academic_researcher, blog_writer, article_evaluator],
        tasks=[research_task, write_task, evaluate_task],
        process=Process.sequential
    )

    user_question = st.text_input("Fa√ßa uma pergunta:")
    if user_question:
        current_prompt = secondary_prompt if 'last_prompt' in st.session_state and st.session_state.last_prompt == primary_prompt else primary_prompt
        st.session_state.last_prompt = current_prompt

        prompt = f"{current_prompt} {user_question}"
        model = crew.agents[0].llm.model_name  # Assume que o modelo do primeiro agente √© o modelo escolhido
        while True:
            # Verificar se excedeu o limite de tokens
            if tokens_used[model] >= rate_limits[model]:
                st.warning(f"Limite de tokens excedido para o modelo {model}. Aguardando antes de tentar novamente...")
                time.sleep(60)
                tokens_used[model] = 0
            else:
                try:
                    result = crew.kickoff(inputs={"topic": user_question})
                    st.write("Chatbot:", result)
                    tokens_used[model] += 1  # Incrementar o contador de tokens
                    break
                except groq.RateLimitError as e:
                    # Extrair o tempo de espera da mensagem de erro
                    retry_time_str = e.args[0]["error"]["message"].split("Please try again in ")[1].split(".")[0]
                    retry_time_seconds = int(retry_time_str.split("m")[0]) * 60 + float(retry_time_str.split("m")[1][:-1])
                    st.warning(f"Limite de taxa excedido. Aguardando {retry_time_seconds} segundos antes de tentar novamente...")
                    time.sleep(retry_time_seconds)

        # Exibir a contagem de tokens na barra lateral
        st.sidebar.write("### Limites de Taxa")
        st.sidebar.write("Aqui est√£o alguns pontos-chave sobre os limites de taxa:")
        st.sidebar.write("Os seguintes cabe√ßalhos s√£o definidos (os valores s√£o ilustrativos):")
        st.sidebar.write("```")
        st.sidebar.write("Cabe√ßalho\tValor\tAnota√ß√µes")
        st.sidebar.write("retry-after\t2\tEm segundos")
        st.sidebar.write("x-ratelimit-limit-requests\t14400\tSempre se refere a Solicita√ß√µes por Dia (RPD)")
        st.sidebar.write("x-ratelimit-limit-tokens\t18000\tSempre se refere a Tokens por Minuto (TPM)")
        st.sidebar.write("x-ratelimit-remaining-requests\t14370\tSempre se refere a Solicita√ß√µes por Dia (RPD)")
        st.sidebar.write("x-ratelimit-remaining-tokens\t17997\tSempre se refere a Tokens por Minuto (TPM)")
        st.sidebar.write("x-ratelimit-reset-requests\t2m59.56s\tSempre se refere a Solicita√ß√µes por Dia (RPD)")
        st.sidebar.write("x-ratelimit-reset-tokens\t7.66s\tSempre se refere a Tokens por Minuto (TPM)")
        st.sidebar.write("```")
        st.sidebar.write("Quando o limite de taxa √© atingido, retornamos um c√≥digo de status HTTP Too Many Requests.429")
    
        # Exibir a tabela de limites de taxa
        st.sidebar.write("### Limites de Taxa")
        st.sidebar.write("Estes s√£o os limites de taxa para sua organiza√ß√£o:")
        st.sidebar.write("```")
        st.sidebar.write("ID\tSolicita√ß√µes por Minuto\tPedidos por Dia\tTokens por Minuto")
        st.sidebar.write("MixTral-8X7B-32768\t30\t14.400\t5.000")
        st.sidebar.write("Lhama3-70B-8192\t30\t14.400\t6.000")
        st.sidebar.write("lhama3-8b-8192\t30\t14.400\t30.000")
        st.sidebar.write("gemma-7b-it\t30\t14.400\t15.000")
        st.sidebar.write("```")
    
        # Contagem de tokens usados
        st.sidebar.write("### Contagem de Tokens Usados")
        for model, tokens in tokens_used.items():
            st.sidebar.write(f"{model}: {tokens}")

        

if __name__ == "__main__":
    main()
