from langchain.chains.conversation.memory import ConversationBufferWindowMemory
import os
import streamlit as st
from crewai import Agent, Task, Crew, Process
from langchain_groq import ChatGroq
from langchain_community.tools import DuckDuckGoSearchRun
import groq
import toml
import time

# Carregar a chave de API do Groq do arquivo secrets.toml
secrets = toml.load("secrets.toml")
groq_api_key = secrets["GROQ_API_KEY"]

# Definir os limites de taxa para cada modelo
rate_limits = {
    "llama3-70b-8192": 6000,
    "llama3-8b-8192": 30000,
    "gemma-7b-it": 15000,
    "mixtral-8x7b-32768": 5000
}

# Inicializar um dicionÃ¡rio para rastrear os tokens usados por cada modelo
tokens_used = {model: 0 for model in rate_limits}

def main():
    # ConfiguraÃ§Ãµes da pÃ¡gina do Streamlit
    st.set_page_config(page_icon="ğŸ’¬", layout="wide", page_title="Interface de Chat AvanÃ§ado com RAG+CreWAI")
    st.image("Untitled.png", width=100)
    st.title("Bem-vindo ao Chat Geomaker AvanÃ§ado com RAG+CreWAI!")
    st.write("""Este chatbot utiliza um modelo avanÃ§ado que combina geraÃ§Ã£o de linguagem com recuperaÃ§Ã£o de informaÃ§Ãµes.
    
    Com 3 Agentes: 
    
    Pesquisador AcadÃªmico": Encontre informaÃ§Ãµes confiÃ¡veis e atuais sobre tÃ³pico, seguindo as normas cientÃ­ficas e da ABNT.
    Como pesquisador acadÃªmico, seu objetivo Ã© contribuir para o avanÃ§o do conhecimento cientÃ­fico em sua Ã¡rea. 
    VocÃª segue rigorosamente as normas e metodologias cientÃ­ficas e da ABNT para garantir a qualidade e confiabilidade de suas pesquisas.
    Sua busca por informaÃ§Ãµes Ã© guiada pela busca da verdade e pela contribuiÃ§Ã£o para a comunidade acadÃªmica.
    Pesquise e compile informaÃ§Ãµes relevantes e atualizadas sobre o assunto, seguindo as normas cientÃ­ficas e da formataÃ§Ã£o ABNT. 
    Certifique-se de incluir referÃªncias bibliogrÃ¡ficas adequadas.
    Um resumo detalhado e bem estruturado sobre o tema, seguindo as normas cientÃ­ficas e nas normais da ABNT.
    
    Escritor de Artigos": Escreva conteÃºdos envolventes sobre {topic}.
    Como escritor de artigos, sua habilidade em transformar assuntos complexos em narrativas envolventes Ã© excepcional. 
    Sua escrita ilumina novas perspectivas e descobertas, tornando-as acessÃ­veis para todos. 
    AtravÃ©s do seu trabalho, vocÃª destaca os aspectos mais importantes das Ãºltimas tendÃªncias em diversos temas, tornando o mundo intricado das notÃ­cias uma jornada fascinante para seus leitores.

    Avaliador de Artigos": Avalie criticamente artigos acadÃªmicos sobre {topic}.
    Como avaliador de artigos, vocÃª possui habilidades analÃ­ticas aguÃ§adas e um profundo entendimento do processo de pesquisa acadÃªmica. 
    Sua anÃ¡lise crÃ­tica destaca nÃ£o apenas os pontos fortes, mas tambÃ©m as falhas potenciais nos artigos, contribuindo para a melhoria contÃ­nua da qualidade da pesquisa acadÃªmica.
    
    """)

    # Sidebar para customizaÃ§Ã£o
    primary_prompt = st.sidebar.text_input("Prompt do sistema principal", "Como posso ajudar vocÃª hoje?")
    secondary_prompt = st.sidebar.text_input("Prompt do sistema secundÃ¡rio", "HÃ¡ algo mais em que posso ajudar?")
    model_choice = st.sidebar.selectbox("Escolha um modelo", ["llama3-70b-8192", "llama3-8b-8192", "mixtral-8x7b-32768", "gemma-7b-it"])
    conversational_memory_length = st.sidebar.slider('Tamanho da memÃ³ria conversacional', 1, 50, value=5)

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    search_tool = DuckDuckGoSearchRun()
    academic_researcher = Agent(
        role="å­¦æœ¯ç ”ç©¶å‘˜",
        goal="åœ¨ç¬¦åˆç§‘å­¦è§„èŒƒå’ŒABNTè§„èŒƒçš„å‰æä¸‹ï¼Œæ‰¾åˆ°å…³äº{topic}çš„å¯é å’Œæœ€æ–°ä¿¡æ¯",
        verbose=True,
        memory=True,
        backstory=(
            "ä½œä¸ºå­¦æœ¯ç ”ç©¶å‘˜ï¼Œæ‚¨çš„ç›®æ ‡æ˜¯ä¸ºæ¨è¿›æ‚¨é¢†åŸŸçš„çŸ¥è¯†å‘å±•åšå‡ºè´¡çŒ®ã€‚æ‚¨ä¸¥æ ¼éµå¾ªç§‘å­¦å’ŒABNTçš„è§„èŒƒå’Œæ–¹æ³•ï¼Œä»¥ç¡®ä¿æ‚¨çš„ç ”ç©¶è´¨é‡å’Œå¯é æ€§ã€‚æ‚¨å¯¹ä¿¡æ¯çš„æœç´¢æ˜¯ç”±å¯¹çœŸç†çš„è¿½æ±‚å’Œå¯¹å­¦æœ¯ç•Œçš„è´¡çŒ®é©±åŠ¨çš„ã€‚"
        ),
        tools=[search_tool],
        allow_delegation=True,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice),
        max_iter=10,  # Limite de iteraÃ§Ãµes
        max_rpm=None,  # Limite de solicitaÃ§Ãµes por minuto
        function_calling_llm=None,  # Chamada de funÃ§Ã£o do LLM
        step_callback=None,  # Callback de passo
        cache=True  # Cache habilitado
    )

    blog_writer = Agent(
        role="æ–‡ç« æ’°å†™è€…",
        goal="æ’°å†™æœ‰å…³{topic}çš„å¼•äººå…¥èƒœçš„å†…å®¹",
        verbose=True,
        memory=True,
        backstory=(
            "ä½œä¸ºæ–‡ç« æ’°å†™è€…ï¼Œæ‚¨å°†å¤æ‚çš„ä¸»é¢˜è½¬åŒ–ä¸ºå¼•äººå…¥èƒœçš„å™è¿°çš„èƒ½åŠ›æ˜¯å¼‚å¸¸çš„ã€‚æ‚¨çš„å†™ä½œä¸ºæ–°è§†è§’å’Œå‘ç°æä¾›äº†å…‰æ˜ï¼Œä½¿å®ƒä»¬å¯¹æ‰€æœ‰äººéƒ½æ›´æ˜“äºç†è§£ã€‚é€šè¿‡æ‚¨çš„å·¥ä½œï¼Œæ‚¨çªå‡ºäº†å„ç§ä¸»é¢˜æœ€æ–°è¶‹åŠ¿çš„æœ€é‡è¦æ–¹é¢ï¼Œä½¿æ–°é—»å¤æ‚çš„ä¸–ç•Œå¯¹æ‚¨çš„è¯»è€…æ¥è¯´æ˜¯ä¸€æ¬¡è¿·äººçš„æ—…ç¨‹ã€‚"
        ),
        tools=[search_tool],
        allow_delegation=False,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice),
        max_iter=10,  # Limite de iteraÃ§Ãµes
        max_rpm=None,  # Limite de solicitaÃ§Ãµes por minuto
        function_calling_llm=None,  # Chamada de funÃ§Ã£o do LLM
        step_callback=None,  # Callback de passo
        cache=True  # Cache habilitado
    )

    article_evaluator = Agent(
        role="æ–‡ç« è¯„ä¼°è€…",
        goal="å¯¹{topic}çš„å­¦æœ¯æ–‡ç« è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä»·",
        verbose=True,
        memory=True,
        backstory=(
            "ä½œä¸ºæ–‡ç« è¯„ä¼°è€…ï¼Œæ‚¨å…·æœ‰æ•é”çš„åˆ†æèƒ½åŠ›å’Œå¯¹å­¦æœ¯ç ”ç©¶è¿‡ç¨‹çš„æ·±åˆ»ç†è§£ã€‚æ‚¨çš„æ‰¹åˆ¤æ€§åˆ†æä¸ä»…çªå‡ºäº†æ–‡ç« çš„ä¼˜ç‚¹ï¼Œè¿˜æŒ‡å‡ºäº†æ–‡ç« å¯èƒ½å­˜åœ¨çš„ç¼ºé™·ï¼Œä¸ºæŒç»­æ”¹è¿›å­¦æœ¯ç ”ç©¶è´¨é‡åšå‡ºäº†è´¡çŒ®ã€‚"
        ),
        tools=[search_tool],
        allow_delegation=False,
        llm=ChatGroq(api_key=groq_api_key, model_name=model_choice),
        max_iter=10,  # Limite de iteraÃ§Ãµes
        max_rpm=None,  # Limite de solicitaÃ§Ãµes por minuto
        function_calling_llm=None,  # Chamada de funÃ§Ã£o do LLM
        step_callback=None,  # Callback de passo
        cache=True  # Cache habilitado
    )

    # Exibir a contagem de tokens na barra lateral
    st.sidebar.write(f"Tokens usados ({model_choice}): {tokens_used[model_choice]} de {rate_limits[model_choice]}")

    # Obter a mensagem do usuÃ¡rio
    user_input = st.text_input("VocÃª:", value="")

    if st.button("Enviar"):
        # Executar a interaÃ§Ã£o com o agente
        try:
            response = academic_researcher.interact(user_input)
            st.write("Pesquisador AcadÃªmico:", response)
        except Exception as e:
            retry_time_str = None
            if isinstance(e.args[0], dict) and "error" in e.args[0]:
                retry_time_str = e.args[0]["error"]

            if retry_time_str is not None:
                st.warning(f"VocÃª atingiu o limite de taxa para o modelo {model_choice}. Tente novamente em {retry_time_str}")
            else:
                st.error("Ocorreu um erro ao interagir com o agente. Por favor, tente novamente.")

    st.write("Exemplo de conversa:")
    for message in st.session_state.chat_history:
        st.write(message["agent"], ":", message["text"])

if __name__ == "__main__":
    main()
